{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code is a reimplementation of work done from this article: https://towardsdatascience.com/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, kernel_num, kernel_size):\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_size = kernel_size        \n",
    "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        image_h, image_w = image.shape\n",
    "        self.image = image\n",
    "        for h in range(image_h-self.kernel_size+1):\n",
    "            for w in range(image_w-self.kernel_size+1):\n",
    "                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "    \n",
    "    def forward_prop(self, image):\n",
    "        image_h, image_w = image.shape\n",
    "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            convolution_output[h,w] = np.sum(patch*self.kernels, axis=(1,2))\n",
    "        return convolution_output\n",
    "    \n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        dE_dk = np.zeros(self.kernels.shape)\n",
    "        for patch, h, w in self.patches_generator(self.image):\n",
    "            for f in range(self.kernel_num):\n",
    "                dE_dk[f] += patch * dE_dY[h, w, f]\n",
    "        self.kernels -= alpha*dE_dk\n",
    "        return dE_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def patches_generator(self, image):\n",
    "        output_h = image.shape[0] // self.kernel_size\n",
    "        output_w = image.shape[1] // self.kernel_size\n",
    "        self.image = image\n",
    "\n",
    "        for h in range(output_h):\n",
    "            for w in range(output_w):\n",
    "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        image_h, image_w, num_kernels = image.shape\n",
    "        max_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
    "        for patch, h, w in self.patches_generator(image):\n",
    "            max_pooling_output[h,w] = np.amax(patch, axis=(0,1))\n",
    "        return max_pooling_output\n",
    "\n",
    "    def back_prop(self, dE_dY):\n",
    "        dE_dk = np.zeros(self.image.shape)\n",
    "        for patch,h,w in self.patches_generator(self.image):\n",
    "            image_h, image_w, num_kernels = patch.shape\n",
    "            max_val = np.amax(patch, axis=(0,1))\n",
    "\n",
    "            for idx_h in range(image_h):\n",
    "                for idx_w in range(image_w):\n",
    "                    for idx_k in range(num_kernels):\n",
    "                        if patch[idx_h,idx_w,idx_k] == max_val[idx_k]:\n",
    "                            dE_dk[h*self.kernel_size+idx_h, w*self.kernel_size+idx_w, idx_k] = dE_dY[h,w,idx_k]\n",
    "            return dE_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
    "        self.bias = np.zeros(output_units)\n",
    "\n",
    "    def forward_prop(self, image):\n",
    "        self.original_shape = image.shape\n",
    "        image_flattened = image.flatten()\n",
    "        self.flattened_input = image_flattened\n",
    "        first_output = np.dot(image_flattened, self.weight) + self.bias\n",
    "        self.output = first_output\n",
    "        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
    "        return softmax_output\n",
    "\n",
    "    def back_prop(self, dE_dY, alpha):\n",
    "        for i, gradient in enumerate(dE_dY):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            transformation_eq = np.exp(self.output)\n",
    "            S_total = np.sum(transformation_eq)\n",
    "\n",
    "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
    "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
    "\n",
    "            dZ_dw = self.flattened_input\n",
    "            dZ_db = 1\n",
    "            dZ_dX = self.weight\n",
    "\n",
    "            dE_dZ = gradient * dY_dZ\n",
    "\n",
    "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
    "            dE_db = dE_dZ * dZ_db\n",
    "            dE_dX = dZ_dX @ dE_dZ\n",
    "\n",
    "            self.weight -= alpha*dE_dw\n",
    "            self.bias -= alpha*dE_db\n",
    "\n",
    "            return dE_dX.reshape(self.original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_forward(image, label, layers):\n",
    "    output = image/255.\n",
    "    for layer in layers:\n",
    "        output = layer.forward_prop(output)\n",
    "    # Compute loss (cross-entropy) and accuracy\n",
    "    loss = -np.log(output[label])\n",
    "    accuracy = 1 if np.argmax(output) == label else 0\n",
    "    return output, loss, accuracy\n",
    "\n",
    "def CNN_backprop(gradient, layers, alpha=0.05):\n",
    "    grad_back = gradient\n",
    "    for layer in layers[::-1]:\n",
    "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
    "            grad_back = layer.back_prop(grad_back, alpha)\n",
    "        elif type(layer) == MaxPoolingLayer:\n",
    "            grad_back = layer.back_prop(grad_back)\n",
    "    return grad_back\n",
    "\n",
    "\n",
    "def CNN_training(image, label, layers, alpha=0.05):\n",
    "    # Forward step\n",
    "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
    "\n",
    "    # Initial gradient\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1/output[label]\n",
    "\n",
    "    # Backprop step\n",
    "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 1.8354008716979409, accuracy 40\n",
      "Step 201. For the last 100 steps: average loss 1.2088999852428497, accuracy 67\n",
      "Step 301. For the last 100 steps: average loss 0.9907515113558675, accuracy 74\n",
      "Step 401. For the last 100 steps: average loss 0.9314542021132928, accuracy 75\n",
      "Step 501. For the last 100 steps: average loss 0.6204044261181664, accuracy 85\n",
      "Step 601. For the last 100 steps: average loss 0.6175164154693005, accuracy 84\n",
      "Step 701. For the last 100 steps: average loss 0.5944846922298216, accuracy 85\n",
      "Step 801. For the last 100 steps: average loss 0.6639757063992533, accuracy 83\n",
      "Step 901. For the last 100 steps: average loss 0.7349301290810469, accuracy 80\n",
      "Step 1001. For the last 100 steps: average loss 0.6973182821186845, accuracy 84\n",
      "Step 1101. For the last 100 steps: average loss 0.576525883565104, accuracy 83\n",
      "Step 1201. For the last 100 steps: average loss 0.507121420956701, accuracy 87\n",
      "Step 1301. For the last 100 steps: average loss 0.4531737297287821, accuracy 85\n",
      "Step 1401. For the last 100 steps: average loss 0.4474759994979467, accuracy 90\n",
      "Step 1501. For the last 100 steps: average loss 0.5812820512791691, accuracy 87\n",
      "Step 1601. For the last 100 steps: average loss 0.45112916143401016, accuracy 89\n",
      "Step 1701. For the last 100 steps: average loss 0.5117719877220309, accuracy 83\n",
      "Step 1801. For the last 100 steps: average loss 0.41797821133124435, accuracy 89\n",
      "Step 1901. For the last 100 steps: average loss 0.4551019675283862, accuracy 88\n",
      "Step 2001. For the last 100 steps: average loss 0.4182229092711682, accuracy 87\n",
      "Step 2101. For the last 100 steps: average loss 0.5884896188213701, accuracy 86\n",
      "Step 2201. For the last 100 steps: average loss 0.4107944758001494, accuracy 90\n",
      "Step 2301. For the last 100 steps: average loss 0.4551013738005611, accuracy 87\n",
      "Step 2401. For the last 100 steps: average loss 0.49380628250327424, accuracy 86\n",
      "Step 2501. For the last 100 steps: average loss 0.35329795083519294, accuracy 90\n",
      "Step 2601. For the last 100 steps: average loss 0.36208508686078283, accuracy 91\n",
      "Step 2701. For the last 100 steps: average loss 0.2832736176942217, accuracy 91\n",
      "Step 2801. For the last 100 steps: average loss 0.5186370800213154, accuracy 81\n",
      "Step 2901. For the last 100 steps: average loss 0.2891494555547745, accuracy 91\n",
      "Step 3001. For the last 100 steps: average loss 0.33224763189087125, accuracy 93\n",
      "Step 3101. For the last 100 steps: average loss 0.38178276874050804, accuracy 89\n",
      "Step 3201. For the last 100 steps: average loss 0.28898262015708914, accuracy 93\n",
      "Step 3301. For the last 100 steps: average loss 0.44898456355680877, accuracy 89\n",
      "Step 3401. For the last 100 steps: average loss 0.49316594533257246, accuracy 87\n",
      "Step 3501. For the last 100 steps: average loss 0.4365277468611188, accuracy 89\n",
      "Step 3601. For the last 100 steps: average loss 0.2859137411370696, accuracy 94\n",
      "Step 3701. For the last 100 steps: average loss 0.3014289100160275, accuracy 93\n",
      "Step 3801. For the last 100 steps: average loss 0.30376419168808083, accuracy 90\n",
      "Step 3901. For the last 100 steps: average loss 0.3687161203152302, accuracy 92\n",
      "Step 4001. For the last 100 steps: average loss 0.31681407515086685, accuracy 94\n",
      "Step 4101. For the last 100 steps: average loss 0.4700010695489026, accuracy 89\n",
      "Step 4201. For the last 100 steps: average loss 0.3344247010602735, accuracy 90\n",
      "Step 4301. For the last 100 steps: average loss 0.4181888222518827, accuracy 86\n",
      "Step 4401. For the last 100 steps: average loss 0.43956742000011056, accuracy 89\n",
      "Step 4501. For the last 100 steps: average loss 0.41026740314655386, accuracy 88\n",
      "Step 4601. For the last 100 steps: average loss 0.4628639279472128, accuracy 83\n",
      "Step 4701. For the last 100 steps: average loss 0.2884923717672718, accuracy 93\n",
      "Step 4801. For the last 100 steps: average loss 0.27046695490069916, accuracy 95\n",
      "Step 4901. For the last 100 steps: average loss 0.29827628501671843, accuracy 90\n",
      "Epoch 2 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.20763085742587414, accuracy 94\n",
      "Step 201. For the last 100 steps: average loss 0.34555736021592715, accuracy 91\n",
      "Step 301. For the last 100 steps: average loss 0.2346957633247298, accuracy 95\n",
      "Step 401. For the last 100 steps: average loss 0.2967050336810205, accuracy 93\n",
      "Step 501. For the last 100 steps: average loss 0.25113670117393566, accuracy 95\n",
      "Step 601. For the last 100 steps: average loss 0.24380430557809882, accuracy 95\n",
      "Step 701. For the last 100 steps: average loss 0.30530208069751336, accuracy 91\n",
      "Step 801. For the last 100 steps: average loss 0.30358577746297327, accuracy 91\n",
      "Step 901. For the last 100 steps: average loss 0.24610457882970743, accuracy 90\n",
      "Step 1001. For the last 100 steps: average loss 0.25194460283765713, accuracy 96\n",
      "Step 1101. For the last 100 steps: average loss 0.3696736174959797, accuracy 94\n",
      "Step 1201. For the last 100 steps: average loss 0.3168938861955667, accuracy 89\n",
      "Step 1301. For the last 100 steps: average loss 0.2585387979706338, accuracy 94\n",
      "Step 1401. For the last 100 steps: average loss 0.18859767493231858, accuracy 93\n",
      "Step 1501. For the last 100 steps: average loss 0.2743921189977367, accuracy 93\n",
      "Step 1601. For the last 100 steps: average loss 0.16374769030858086, accuracy 97\n",
      "Step 1701. For the last 100 steps: average loss 0.2157585675445731, accuracy 94\n",
      "Step 1801. For the last 100 steps: average loss 0.26628144387012337, accuracy 92\n",
      "Step 1901. For the last 100 steps: average loss 0.3432062901438163, accuracy 89\n",
      "Step 2001. For the last 100 steps: average loss 0.28502907590578613, accuracy 92\n",
      "Step 2101. For the last 100 steps: average loss 0.36309581801364244, accuracy 88\n",
      "Step 2201. For the last 100 steps: average loss 0.28478130986779887, accuracy 93\n",
      "Step 2301. For the last 100 steps: average loss 0.3358942835104295, accuracy 94\n",
      "Step 2401. For the last 100 steps: average loss 0.5058314618923554, accuracy 83\n",
      "Step 2501. For the last 100 steps: average loss 0.29717906254264803, accuracy 93\n",
      "Step 2601. For the last 100 steps: average loss 0.3379700533988036, accuracy 86\n",
      "Step 2701. For the last 100 steps: average loss 0.2560822228260099, accuracy 94\n",
      "Step 2801. For the last 100 steps: average loss 0.17474202539695147, accuracy 96\n",
      "Step 2901. For the last 100 steps: average loss 0.30620465211423814, accuracy 88\n",
      "Step 3001. For the last 100 steps: average loss 0.26656688825355773, accuracy 92\n",
      "Step 3101. For the last 100 steps: average loss 0.41707458283172416, accuracy 83\n",
      "Step 3201. For the last 100 steps: average loss 0.5487881713057488, accuracy 88\n",
      "Step 3301. For the last 100 steps: average loss 0.1580984448925998, accuracy 98\n",
      "Step 3401. For the last 100 steps: average loss 0.31669933432979536, accuracy 92\n",
      "Step 3501. For the last 100 steps: average loss 0.26162897633764964, accuracy 94\n",
      "Step 3601. For the last 100 steps: average loss 0.24991807307226885, accuracy 94\n",
      "Step 3701. For the last 100 steps: average loss 0.30736238916134306, accuracy 93\n",
      "Step 3801. For the last 100 steps: average loss 0.23028521593839715, accuracy 93\n",
      "Step 3901. For the last 100 steps: average loss 0.281241699928827, accuracy 93\n",
      "Step 4001. For the last 100 steps: average loss 0.27282908085716606, accuracy 93\n",
      "Step 4101. For the last 100 steps: average loss 0.2670425877431728, accuracy 93\n",
      "Step 4201. For the last 100 steps: average loss 0.20646809158473464, accuracy 95\n",
      "Step 4301. For the last 100 steps: average loss 0.2896328184848747, accuracy 97\n",
      "Step 4401. For the last 100 steps: average loss 0.3527962659721638, accuracy 90\n",
      "Step 4501. For the last 100 steps: average loss 0.42317380856513437, accuracy 89\n",
      "Step 4601. For the last 100 steps: average loss 0.4284859945492947, accuracy 86\n",
      "Step 4701. For the last 100 steps: average loss 0.24744891817840373, accuracy 93\n",
      "Step 4801. For the last 100 steps: average loss 0.21894753970359024, accuracy 94\n",
      "Step 4901. For the last 100 steps: average loss 0.2369512853228543, accuracy 93\n",
      "Epoch 3 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.3206307300109122, accuracy 86\n",
      "Step 201. For the last 100 steps: average loss 0.2399006856587568, accuracy 96\n",
      "Step 301. For the last 100 steps: average loss 0.31450184793413194, accuracy 94\n",
      "Step 401. For the last 100 steps: average loss 0.22658971550145593, accuracy 95\n",
      "Step 501. For the last 100 steps: average loss 0.23767878643931478, accuracy 96\n",
      "Step 601. For the last 100 steps: average loss 0.23659508329105747, accuracy 93\n",
      "Step 701. For the last 100 steps: average loss 0.13515417075528835, accuracy 97\n",
      "Step 801. For the last 100 steps: average loss 0.26407254791201895, accuracy 97\n",
      "Step 901. For the last 100 steps: average loss 0.23635129156650764, accuracy 95\n",
      "Step 1001. For the last 100 steps: average loss 0.3550300659145932, accuracy 89\n",
      "Step 1101. For the last 100 steps: average loss 0.1783987590339514, accuracy 97\n",
      "Step 1201. For the last 100 steps: average loss 0.3324046836921516, accuracy 93\n",
      "Step 1301. For the last 100 steps: average loss 0.2036708288274466, accuracy 97\n",
      "Step 1401. For the last 100 steps: average loss 0.16717920110050738, accuracy 96\n",
      "Step 1501. For the last 100 steps: average loss 0.1966231036209348, accuracy 96\n",
      "Step 1601. For the last 100 steps: average loss 0.18504682751864468, accuracy 93\n",
      "Step 1701. For the last 100 steps: average loss 0.3285459620885343, accuracy 89\n",
      "Step 1801. For the last 100 steps: average loss 0.16808556415185666, accuracy 98\n",
      "Step 1901. For the last 100 steps: average loss 0.10193097713343063, accuracy 98\n",
      "Step 2001. For the last 100 steps: average loss 0.409467399556798, accuracy 87\n",
      "Step 2101. For the last 100 steps: average loss 0.30975789846375473, accuracy 89\n",
      "Step 2201. For the last 100 steps: average loss 0.1907586688651484, accuracy 96\n",
      "Step 2301. For the last 100 steps: average loss 0.14551480717602577, accuracy 97\n",
      "Step 2401. For the last 100 steps: average loss 0.23977095146750343, accuracy 93\n",
      "Step 2501. For the last 100 steps: average loss 0.2230223004509916, accuracy 94\n",
      "Step 2601. For the last 100 steps: average loss 0.2048853209989492, accuracy 95\n",
      "Step 2701. For the last 100 steps: average loss 0.2722338711293125, accuracy 94\n",
      "Step 2801. For the last 100 steps: average loss 0.191103412732691, accuracy 96\n",
      "Step 2901. For the last 100 steps: average loss 0.26197115450225933, accuracy 95\n",
      "Step 3001. For the last 100 steps: average loss 0.3031082751513673, accuracy 90\n",
      "Step 3101. For the last 100 steps: average loss 0.27875129023222445, accuracy 91\n",
      "Step 3201. For the last 100 steps: average loss 0.16635019424665548, accuracy 95\n",
      "Step 3301. For the last 100 steps: average loss 0.22870566456347252, accuracy 94\n",
      "Step 3401. For the last 100 steps: average loss 0.3170031315855068, accuracy 92\n",
      "Step 3501. For the last 100 steps: average loss 0.2582574776277762, accuracy 94\n",
      "Step 3601. For the last 100 steps: average loss 0.1841889892407733, accuracy 96\n",
      "Step 3701. For the last 100 steps: average loss 0.2986722656544834, accuracy 91\n",
      "Step 3801. For the last 100 steps: average loss 0.3788305376655871, accuracy 89\n",
      "Step 3901. For the last 100 steps: average loss 0.17702299185251547, accuracy 93\n",
      "Step 4001. For the last 100 steps: average loss 0.11424900847797619, accuracy 98\n",
      "Step 4101. For the last 100 steps: average loss 0.18474005919652065, accuracy 95\n",
      "Step 4201. For the last 100 steps: average loss 0.23841499102079028, accuracy 95\n",
      "Step 4301. For the last 100 steps: average loss 0.2998091048547565, accuracy 91\n",
      "Step 4401. For the last 100 steps: average loss 0.249845479964673, accuracy 94\n",
      "Step 4501. For the last 100 steps: average loss 0.24919292676479413, accuracy 94\n",
      "Step 4601. For the last 100 steps: average loss 0.30914966083434686, accuracy 90\n",
      "Step 4701. For the last 100 steps: average loss 0.19207051901578265, accuracy 95\n",
      "Step 4801. For the last 100 steps: average loss 0.18041606515594208, accuracy 96\n",
      "Step 4901. For the last 100 steps: average loss 0.20232308812420818, accuracy 95\n",
      "Epoch 4 ->\n",
      "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
      "Step 101. For the last 100 steps: average loss 0.15983628318000886, accuracy 96\n",
      "Step 201. For the last 100 steps: average loss 0.25110164369981725, accuracy 93\n",
      "Step 301. For the last 100 steps: average loss 0.224180926152505, accuracy 94\n",
      "Step 401. For the last 100 steps: average loss 0.19360047170823796, accuracy 95\n",
      "Step 501. For the last 100 steps: average loss 0.21694643937840874, accuracy 96\n",
      "Step 601. For the last 100 steps: average loss 0.16379103641351064, accuracy 96\n",
      "Step 701. For the last 100 steps: average loss 0.1873005710376753, accuracy 94\n",
      "Step 801. For the last 100 steps: average loss 0.19044283725312267, accuracy 94\n",
      "Step 901. For the last 100 steps: average loss 0.14293883396387858, accuracy 97\n",
      "Step 1001. For the last 100 steps: average loss 0.1645460613885439, accuracy 95\n",
      "Step 1101. For the last 100 steps: average loss 0.2119528675839781, accuracy 95\n",
      "Step 1201. For the last 100 steps: average loss 0.34086979979813026, accuracy 92\n",
      "Step 1301. For the last 100 steps: average loss 0.17763715500041932, accuracy 95\n",
      "Step 1401. For the last 100 steps: average loss 0.07959079498557055, accuracy 99\n",
      "Step 1501. For the last 100 steps: average loss 0.12086669750072838, accuracy 97\n",
      "Step 1601. For the last 100 steps: average loss 0.19254861541226473, accuracy 95\n",
      "Step 1701. For the last 100 steps: average loss 0.14473212583663775, accuracy 98\n",
      "Step 1801. For the last 100 steps: average loss 0.1447280655721005, accuracy 96\n",
      "Step 1901. For the last 100 steps: average loss 0.22740009905545785, accuracy 94\n",
      "Step 2001. For the last 100 steps: average loss 0.1728309500972419, accuracy 96\n",
      "Step 2101. For the last 100 steps: average loss 0.20208848362129062, accuracy 95\n",
      "Step 2201. For the last 100 steps: average loss 0.3144407239062039, accuracy 92\n",
      "Step 2301. For the last 100 steps: average loss 0.2739206507610419, accuracy 91\n",
      "Step 2401. For the last 100 steps: average loss 0.1461817708547011, accuracy 97\n",
      "Step 2501. For the last 100 steps: average loss 0.22745758499646734, accuracy 92\n",
      "Step 2601. For the last 100 steps: average loss 0.18475705581093252, accuracy 96\n",
      "Step 2701. For the last 100 steps: average loss 0.18737593161857977, accuracy 96\n",
      "Step 2801. For the last 100 steps: average loss 0.13793298059032652, accuracy 95\n",
      "Step 2901. For the last 100 steps: average loss 0.3014920573198654, accuracy 93\n",
      "Step 3001. For the last 100 steps: average loss 0.3660038900892305, accuracy 91\n",
      "Step 3101. For the last 100 steps: average loss 0.32868761412044983, accuracy 92\n",
      "Step 3201. For the last 100 steps: average loss 0.12335271160365911, accuracy 97\n",
      "Step 3301. For the last 100 steps: average loss 0.1091542670708861, accuracy 97\n",
      "Step 3401. For the last 100 steps: average loss 0.1281834734146577, accuracy 95\n",
      "Step 3501. For the last 100 steps: average loss 0.23729951390799897, accuracy 95\n",
      "Step 3601. For the last 100 steps: average loss 0.17752404067064362, accuracy 94\n",
      "Step 3701. For the last 100 steps: average loss 0.2456164669560696, accuracy 91\n",
      "Step 3801. For the last 100 steps: average loss 0.12709246121425383, accuracy 96\n",
      "Step 3901. For the last 100 steps: average loss 0.16336028761936996, accuracy 96\n",
      "Step 4001. For the last 100 steps: average loss 0.1841224570803087, accuracy 95\n",
      "Step 4101. For the last 100 steps: average loss 0.3762139057397841, accuracy 92\n",
      "Step 4201. For the last 100 steps: average loss 0.23742042427259133, accuracy 94\n",
      "Step 4301. For the last 100 steps: average loss 0.18122047347596645, accuracy 93\n",
      "Step 4401. For the last 100 steps: average loss 0.2914953222760983, accuracy 91\n",
      "Step 4501. For the last 100 steps: average loss 0.26885368268621074, accuracy 93\n",
      "Step 4601. For the last 100 steps: average loss 0.11911104246944954, accuracy 97\n",
      "Step 4701. For the last 100 steps: average loss 0.23120571431984085, accuracy 95\n",
      "Step 4801. For the last 100 steps: average loss 0.20745884807609558, accuracy 94\n",
      "Step 4901. For the last 100 steps: average loss 0.26648842235460146, accuracy 91\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "  # Load training data\n",
    "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "  X_train = X_train[:5000]\n",
    "  y_train = y_train[:5000]\n",
    "\n",
    "  # Define the network\n",
    "  layers = [\n",
    "    ConvolutionLayer(16,3), # layer with 8 3x3 filters, output (26,26,16)\n",
    "    MaxPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
    "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 output\n",
    "    ] \n",
    "\n",
    "  for epoch in range(4):\n",
    "    print('Epoch {} ->'.format(epoch+1))\n",
    "    # Shuffle training data\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train = X_train[permutation]\n",
    "    y_train = y_train[permutation]\n",
    "    # Training the CNN\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
    "      if i % 100 == 0: # Every 100 examples\n",
    "        print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "      loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
    "      loss += loss_1\n",
    "      accuracy += accuracy_1\n",
    "  \n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
